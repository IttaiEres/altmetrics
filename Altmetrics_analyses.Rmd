---
title: "On the origin of citations"
author: "Ittai Eres"
date: "September 15, 2015"
output:
  html_document:
    fig_caption: yes
    fig_height: 8
    fig_width: 8
    highlight: espresso
    number_sections: yes
    self_contained: no
    theme: cerulean
    toc: yes
  word_document: default
---

#Load the data, suckaaaaaaa!
##using read.delim
```{r load_data}
counts_raw <- read.delim("data/counts-raw.txt.gz")
counts_norm <- read.delim("data/counts-norm.txt.gz")
```
#Data exploration.
What's the distributiuon of authors in all articles of our data set?
```{r author_histogram, fig.cap="Figure 1: Number of Authors per Article", echo=FALSE}
hist(counts_raw$authorsCount, main= "Authors per paper", xlab="# authors", ylab="# papers", breaks=25, xlim=c(0,25))
```

How many facebook likes do articles tend to get?
```{r facebook_likes_histogram, echo=FALSE}
hist(counts_raw$facebookShareCount, main="FB Shares per paper", xlab="Shares", ylab="# papers", breaks = 5000, xlim=c(0,25))
```

The average number of Facebook shares per paper in the data set is `r mean(counts_raw$facebookShareCount)`

Which row has the paper with the most shares?
`r which.max(counts_raw$facebookShareCount)`

The paper with the most shares is "`r counts_raw[which.max(counts_raw$facebookShareCount), "title"]`"

You can use `render("name_of_markdown_file")` to make an html knit without it popping up.

#Practice with for loops.
```{r, include=FALSE}
for(i in c("cat", "dog", "mouse")){
  cat(i, " dies \n")
}

x <- numeric(length=length(counts_raw$wosCountThru2011))
for(i in 1:length(counts_raw$wosCountThru2011)){
  #x <- c(x, counts_raw$wosCountThru2011 +1) #Adding things on to a vector is slow.
  x[i] <- counts_raw$wosCountThru2011[i]+1 #It's faster to index into the list. Significantly.
}

levels(counts_raw$journal)
results <- numeric(length=length(levels(counts_raw$journal)))
names(results) <-levels(counts_raw$journal)
for(journal in levels(counts_raw$journal)){
  results[journal] <- mean(counts_raw$wosCountThru2011[counts_raw$journal == journal])
}
results
```

#Afternoon session with JD Blischak.
```{r dplyr}
library("dplyr")
```

```{r dplyr usage}
research <- filter(counts_raw, articleType == "Research Article")
research_2006 <- filter(research, year == 2006)
nrow(research_2006)
research_2006_fb <- filter(research, year==2006, facebookCommentCount>0)
nrow(research_2006_fb)

research_2006_fb_tweet_disease <- filter(research, year==2006, facebookCommentCount >0 | backtweetsCount >0, grepl("Infectious Diseases", plosSubjectTags))
nrow(research_2006_fb_tweet_disease)
article_info <- select(research, doi:authorsCount)
colnames(article_info)
metrics <- select(research, contains("Count"), -authorsCount, f1000Factor, wikipediaCites)
colnames(metrics)

head(select(research, journal))
head(select(research, 3))
slice(article_info, 1:3)

low_cite <- select(filter(research, year <= 2008, pdfDownloadsCount > 1000, mendeleyReadersCount > 15, wosCountThru2011 < 10), journal, title, year)
```

#Advancing forward! Chaining commands with dplyr
```{r Chaining commands with dplyr}
#pipe character %>%
research %>% filter(year==2006) %>% select(contains("facebook")) %>% head
research %>% arrange(authorsCount, wosCountThru2011) %>% select(authorsCount, wosCountThru2011)
research %>%
  arrange(desc(authorsCount), desc(wosCountThru2011)) %>%
  select(authorsCount, wosCountThru2011) %>%
  slice(1:10)

#Using a chain of pipes, output the titles of the three research articles with the largest 2011 citation count.
research %>% arrange(desc(wosCountThru2011)) %>% select(title) %>% slice(1:3)

#Using a chain of pipes, output the author count, title, journal, and subject tags (plosSubjectTags) of the three research articles with the largest number of authors.
research %>% arrange(desc(authorsCount)) %>% select(authorsCount, title, journal, plosSubjectTags) %>% slice(1:3)

## Summarizing with dplyr
research <- research %>% mutate(weeksSincePublished = daysSincePublished / 7, yearsSincePublished=weeksSincePublished /52)
research %>% select(contains("Since")) %>% slice(1:10)

research %>% summarize(plos_mean = mean(plosCommentCount))
summarize(research, plos_mean = mean(plosCommentCount), plos_sd = sd(plosCommentCount), num =n())

research %>% group_by(journal) %>% summarize(tweets_mean = mean(backtweetsCount))
research %>% group_by(journal, year) %>% summarize(tweets_mean = mean(backtweetsCount))

tweets_per_journal <- research %>% group_by(journal) %>% summarize(total_articles = n(), tweets_mean=mean(backtweetsCount), SEM = sd(backtweetsCount)/sqrt(total_articles))
```

# Utilizing ggplot2
```{r ggplot2}
library("ggplot2")
p <- ggplot(data = research, mapping = aes(x = pdfDownloadsCount, y = wosCountThru2011))
p + geom_point()

p <- ggplot(research, aes(x = pdfDownloadsCount, y = wosCountThru2011)) + geom_point(aes(color=journal, size=authorsCount, alpha=daysSincePublished)) + geom_smooth()
p

#Can also move particular variables from aes in different layers to the base ggplot command in order to make it part of the base plot and included in future ones. Look at difference when color is included there, for instance:
p <- ggplot(research, aes(x = pdfDownloadsCount, y = wosCountThru2011, color=journal)) + geom_point(aes(size=authorsCount, alpha=daysSincePublished)) + geom_smooth()
p

#Create a scatter plot with daysSincePublished mapped to the x-axis and wosCountThru2011 mapped to the y-axis. Include a loess fit of the data. Set the transparency level (alpha) of the points to 0.5 and color the points according to the journal where the article was published. Make the loess curve red.
p <- ggplot(research, aes(x = daysSincePublished, y = wosCountThru2011)) + geom_point(aes(color=journal), alpha=0.5) + geom_smooth(color="red")
p
```

# Using scales
```{r}
p <- ggplot(research, aes(x=pdfDownloadsCount, y=wosCountThru2011)) + geom_point(aes(color=journal)) + geom_smooth()
p + scale_x_log10() + scale_y_log10()

#Or, if I wanna do the transformation and manage to keep a loess curve:
p <- ggplot(research, aes(x=log10(pdfDownloadsCount+1), y=log10(wosCountThru2011+1))) + geom_point(aes(color=journal)) + geom_smooth()
p

#More fucking around:
p <- ggplot(research, aes(x=log10(pdfDownloadsCount+1), y=log10(wosCountThru2011+1))) + geom_point(aes(color=journal)) + geom_smooth() + scale_x_continuous(breaks=c(1,3), labels=c(10, 1000)) + scale_y_continuous(breaks=c(1,3), labels=c(10, 1000), limits = c(1,3))
p
```

#Different color options.
```{r MOAR colors and subplotting}
p + scale_color_grey()
p + scale_color_manual(values = c("red", "green", "blue", "orange", "pink", "yellow", "purple"))
library("RColorBrewer")
display.brewer.all(type="qual")
p + scale_color_brewer(palette= "Dark2", labels=1:7, name="PLOS")
```
#Challenge!
```{r EVEN MOAR COLORS challenge}
p <- ggplot(research, aes(x = sqrt(pdfDownloadsCount), y = sqrt(wosCountThru2011))) + geom_point(aes(color=journal)) + geom_smooth() + scale_color_brewer(palette="Accent")
p

#Could also have done scale_x_sqrt and scale_y_sqrt instead of putting sqrt inside the aes call of ggplot!
```

```{r SUBPLOTTING}
#Some sweet subplotting shit.
p + facet_wrap(~journal, ncol=2)

research <- mutate(research, immuno=grepl("Immunology", plosSubjectTags))
p <- ggplot(research, aes(x = sqrt(pdfDownloadsCount), y = sqrt(wosCountThru2011))) + geom_point(aes(color=journal)) + geom_smooth() + scale_color_brewer(palette="Accent")
p + facet_grid(journal~immuno)
```

#Using different geoms.
```{r Using different geoms}
p <- ggplot(research, aes(x=journal, y=sqrt(wosCountThru2011))) + geom_boxplot()
p

tweets_per_journal1 <- research %>% group_by(journal) %>% summarize(total_articles = n(), tweets_mean=mean(backtweetsCount), SEM = sd(backtweetsCount)/sqrt(total_articles))

tweets_per_journal

tweets_bar <- ggplot(tweets_per_journal, aes(x=journal, y=mean)) + geom_bar(stat="identity") + geom_errorbar(aes(ymin = (mean-SEM), ymax=(mean +SEM)))

tweets_bar

tweets_per_journal
tweets_per_journal2 <- research %>%
  group_by(journal) %>%
  summarize(num = n(),
            mean = mean(backtweetsCount),
            sem = sd(backtweetsCount) / sqrt(num))
tweets_per_journal

tweets_bar <- ggplot(tweets_per_journal1, aes(x=journal, y=tweets_mean)) + geom_bar(stat="identity") + geom_errorbar(aes(ymin = (tweets_mean-SEM), ymax=(tweets_mean +SEM), width=0.1)) + geom_text(aes(label=total_articles), hjust=0, vjust=0)
tweets_bar
```

```{r CHALLENGE for different geoms}
#Modify the dplyr code above to calculate the mean, SEM, and sample size of the number of article tweets per journal and per year. Use facet_wrap to make a separate subplot per year.
tweets_per_journal_per_year <- research %>% group_by(journal, year) %>% summarize(total_articles = n(), tweets_mean=mean(backtweetsCount), SEM = sd(backtweetsCount)/sqrt(total_articles))

tweets_pjpy <- ggplot(tweets_per_journal_per_year, aes(x=journal, y=tweets_mean)) + geom_bar(stat="identity") + geom_errorbar(aes(ymin = (tweets_mean-SEM), ymax=(tweets_mean +SEM), width=0.1))
tweets_full <- tweets_pjpy + facet_wrap(~year)

tweets_pjpy <- ggplot(tweets_per_journal_per_year, aes(x=journal, y=tweets_mean)) + geom_point() + geom_errorbar(aes(ymin = (tweets_mean-SEM), ymax=(tweets_mean +SEM), width=0.1))
tweets_pjpy + facet_wrap(~year)
```

#Customizing plots.
```{r Customizing plots!}
tweets_full + labs(title= "Mean tweets per journal per year", x="Journal", y="Mean # tweets")
tweets_full + theme_bw()
tweets_full + theme_classic()
tweets_full + theme_minimal()
```

