{
    "contents" : "---\ntitle: \"On the origin of citations\"\nauthor: \"Ittai Eres\"\ndate: \"September 15, 2015\"\noutput:\n  html_document:\n    fig_caption: yes\n    fig_height: 8\n    fig_width: 8\n    highlight: espresso\n    number_sections: yes\n    self_contained: no\n    theme: cerulean\n    toc: yes\n  word_document: default\n---\n\n#Load the data, suckaaaaaaa!\n##using read.delim\n```{r load_data}\ncounts_raw <- read.delim(\"data/counts-raw.txt.gz\")\ncounts_norm <- read.delim(\"data/counts-norm.txt.gz\")\n```\n#Data exploration.\nWhat's the distributiuon of authors in all articles of our data set?\n```{r author_histogram, fig.cap=\"Figure 1: Number of Authors per Article\", echo=FALSE}\nhist(counts_raw$authorsCount, main= \"Authors per paper\", xlab=\"# authors\", ylab=\"# papers\", breaks=25, xlim=c(0,25))\n```\n\nHow many facebook likes do articles tend to get?\n```{r facebook_likes_histogram, echo=FALSE}\nhist(counts_raw$facebookShareCount, main=\"FB Shares per paper\", xlab=\"Shares\", ylab=\"# papers\", breaks = 5000, xlim=c(0,25))\n```\n\nThe average number of Facebook shares per paper in the data set is `r mean(counts_raw$facebookShareCount)`\n\nWhich row has the paper with the most shares?\n`r which.max(counts_raw$facebookShareCount)`\n\nThe paper with the most shares is \"`r counts_raw[which.max(counts_raw$facebookShareCount), \"title\"]`\"\n\nYou can use `render(\"name_of_markdown_file\")` to make an html knit without it popping up.\n\n#Practice with for loops.\n```{r, include=FALSE}\nfor(i in c(\"cat\", \"dog\", \"mouse\")){\n  cat(i, \" dies \\n\")\n}\n\nx <- numeric(length=length(counts_raw$wosCountThru2011))\nfor(i in 1:length(counts_raw$wosCountThru2011)){\n  #x <- c(x, counts_raw$wosCountThru2011 +1) #Adding things on to a vector is slow.\n  x[i] <- counts_raw$wosCountThru2011[i]+1 #It's faster to index into the list. Significantly.\n}\n\nlevels(counts_raw$journal)\nresults <- numeric(length=length(levels(counts_raw$journal)))\nnames(results) <-levels(counts_raw$journal)\nfor(journal in levels(counts_raw$journal)){\n  results[journal] <- mean(counts_raw$wosCountThru2011[counts_raw$journal == journal])\n}\nresults\n```\n\n#Afternoon session with JD Blischak.\n```{r dplyr}\nlibrary(\"dplyr\")\n```\n\n```{r dplyr usage}\nresearch <- filter(counts_raw, articleType == \"Research Article\")\nresearch_2006 <- filter(research, year == 2006)\nnrow(research_2006)\nresearch_2006_fb <- filter(research, year==2006, facebookCommentCount>0)\nnrow(research_2006_fb)\n\nresearch_2006_fb_tweet_disease <- filter(research, year==2006, facebookCommentCount >0 | backtweetsCount >0, grepl(\"Infectious Diseases\", plosSubjectTags))\nnrow(research_2006_fb_tweet_disease)\narticle_info <- select(research, doi:authorsCount)\ncolnames(article_info)\nmetrics <- select(research, contains(\"Count\"), -authorsCount, f1000Factor, wikipediaCites)\ncolnames(metrics)\n\nhead(select(research, journal))\nhead(select(research, 3))\nslice(article_info, 1:3)\n\nlow_cite <- select(filter(research, year <= 2008, pdfDownloadsCount > 1000, mendeleyReadersCount > 15, wosCountThru2011 < 10), journal, title, year)\n```\n\n#Advancing forward! Chaining commands with dplyr\n```{r Chaining commands with dplyr}\n#pipe character %>%\nresearch %>% filter(year==2006) %>% select(contains(\"facebook\")) %>% head\nresearch %>% arrange(authorsCount, wosCountThru2011) %>% select(authorsCount, wosCountThru2011)\nresearch %>%\n  arrange(desc(authorsCount), desc(wosCountThru2011)) %>%\n  select(authorsCount, wosCountThru2011) %>%\n  slice(1:10)\n\n#Using a chain of pipes, output the titles of the three research articles with the largest 2011 citation count.\nresearch %>% arrange(desc(wosCountThru2011)) %>% select(title) %>% slice(1:3)\n\n#Using a chain of pipes, output the author count, title, journal, and subject tags (plosSubjectTags) of the three research articles with the largest number of authors.\nresearch %>% arrange(desc(authorsCount)) %>% select(authorsCount, title, journal, plosSubjectTags) %>% slice(1:3)\n\n## Summarizing with dplyr\nresearch <- research %>% mutate(weeksSincePublished = daysSincePublished / 7, yearsSincePublished=weeksSincePublished /52)\nresearch %>% select(contains(\"Since\")) %>% slice(1:10)\n\nresearch %>% summarize(plos_mean = mean(plosCommentCount))\nsummarize(research, plos_mean = mean(plosCommentCount), plos_sd = sd(plosCommentCount), num =n())\n\nresearch %>% group_by(journal) %>% summarize(tweets_mean = mean(backtweetsCount))\nresearch %>% group_by(journal, year) %>% summarize(tweets_mean = mean(backtweetsCount))\n\ntweets_per_journal <- research %>% group_by(journal) %>% summarize(total_articles = n(), tweets_mean=mean(backtweetsCount), SEM = sd(backtweetsCount)/sqrt(total_articles))\n```\n\n# Utilizing ggplot2\n```{r ggplot2}\nlibrary(\"ggplot2\")\np <- ggplot(data = research, mapping = aes(x = pdfDownloadsCount, y = wosCountThru2011))\np + geom_point()\n\np <- ggplot(research, aes(x = pdfDownloadsCount, y = wosCountThru2011)) + geom_point(aes(color=journal, size=authorsCount, alpha=daysSincePublished)) + geom_smooth()\np\n\n#Can also move particular variables from aes in different layers to the base ggplot command in order to make it part of the base plot and included in future ones. Look at difference when color is included there, for instance:\np <- ggplot(research, aes(x = pdfDownloadsCount, y = wosCountThru2011, color=journal)) + geom_point(aes(size=authorsCount, alpha=daysSincePublished)) + geom_smooth()\np\n\n#Create a scatter plot with daysSincePublished mapped to the x-axis and wosCountThru2011 mapped to the y-axis. Include a loess fit of the data. Set the transparency level (alpha) of the points to 0.5 and color the points according to the journal where the article was published. Make the loess curve red.\np <- ggplot(research, aes(x = daysSincePublished, y = wosCountThru2011)) + geom_point(aes(color=journal), alpha=0.5) + geom_smooth(color=\"red\")\np\n```\n\n# Using scales\n```{r}\np <- ggplot(research, aes(x=pdfDownloadsCount, y=wosCountThru2011)) + geom_point(aes(color=journal)) + geom_smooth()\np + scale_x_log10() + scale_y_log10()\n\n#Or, if I wanna do the transformation and manage to keep a loess curve:\np <- ggplot(research, aes(x=log10(pdfDownloadsCount+1), y=log10(wosCountThru2011+1))) + geom_point(aes(color=journal)) + geom_smooth()\np\n\n#More fucking around:\np <- ggplot(research, aes(x=log10(pdfDownloadsCount+1), y=log10(wosCountThru2011+1))) + geom_point(aes(color=journal)) + geom_smooth() + scale_x_continuous(breaks=c(1,3), labels=c(10, 1000)) + scale_y_continuous(breaks=c(1,3), labels=c(10, 1000), limits = c(1,3))\np\n```\n\n#Different color options.\n```{r MOAR colors and subplotting}\np + scale_color_grey()\np + scale_color_manual(values = c(\"red\", \"green\", \"blue\", \"orange\", \"pink\", \"yellow\", \"purple\"))\nlibrary(\"RColorBrewer\")\ndisplay.brewer.all(type=\"qual\")\np + scale_color_brewer(palette= \"Dark2\", labels=1:7, name=\"PLOS\")\n```\n#Challenge!\n```{r EVEN MOAR COLORS challenge}\np <- ggplot(research, aes(x = sqrt(pdfDownloadsCount), y = sqrt(wosCountThru2011))) + geom_point(aes(color=journal)) + geom_smooth() + scale_color_brewer(palette=\"Accent\")\np\n\n#Could also have done scale_x_sqrt and scale_y_sqrt instead of putting sqrt inside the aes call of ggplot!\n```\n\n```{r SUBPLOTTING}\n#Some sweet subplotting shit.\np + facet_wrap(~journal, ncol=2)\n\nresearch <- mutate(research, immuno=grepl(\"Immunology\", plosSubjectTags))\np <- ggplot(research, aes(x = sqrt(pdfDownloadsCount), y = sqrt(wosCountThru2011))) + geom_point(aes(color=journal)) + geom_smooth() + scale_color_brewer(palette=\"Accent\")\np + facet_grid(journal~immuno)\n```\n\n#Using different geoms.\n```{r Using different geoms}\np <- ggplot(research, aes(x=journal, y=sqrt(wosCountThru2011))) + geom_boxplot()\np\n\ntweets_per_journal1 <- research %>% group_by(journal) %>% summarize(total_articles = n(), tweets_mean=mean(backtweetsCount), SEM = sd(backtweetsCount)/sqrt(total_articles))\n\ntweets_per_journal\n\ntweets_bar <- ggplot(tweets_per_journal, aes(x=journal, y=mean)) + geom_bar(stat=\"identity\") + geom_errorbar(aes(ymin = (mean-SEM), ymax=(mean +SEM)))\n\ntweets_bar\n\ntweets_per_journal\ntweets_per_journal2 <- research %>%\n  group_by(journal) %>%\n  summarize(num = n(),\n            mean = mean(backtweetsCount),\n            sem = sd(backtweetsCount) / sqrt(num))\ntweets_per_journal\n\ntweets_bar <- ggplot(tweets_per_journal1, aes(x=journal, y=tweets_mean)) + geom_bar(stat=\"identity\") + geom_errorbar(aes(ymin = (tweets_mean-SEM), ymax=(tweets_mean +SEM), width=0.1)) + geom_text(aes(label=total_articles), hjust=0, vjust=0)\ntweets_bar\n```\n\n```{r CHALLENGE for different geoms}\n#Modify the dplyr code above to calculate the mean, SEM, and sample size of the number of article tweets per journal and per year. Use facet_wrap to make a separate subplot per year.\ntweets_per_journal_per_year <- research %>% group_by(journal, year) %>% summarize(total_articles = n(), tweets_mean=mean(backtweetsCount), SEM = sd(backtweetsCount)/sqrt(total_articles))\n\ntweets_pjpy <- ggplot(tweets_per_journal_per_year, aes(x=journal, y=tweets_mean)) + geom_bar(stat=\"identity\") + geom_errorbar(aes(ymin = (tweets_mean-SEM), ymax=(tweets_mean +SEM), width=0.1))\ntweets_full <- tweets_pjpy + facet_wrap(~year)\n\ntweets_pjpy <- ggplot(tweets_per_journal_per_year, aes(x=journal, y=tweets_mean)) + geom_point() + geom_errorbar(aes(ymin = (tweets_mean-SEM), ymax=(tweets_mean +SEM), width=0.1))\ntweets_pjpy + facet_wrap(~year)\n```\n\n#Customizing plots.\n```{r Customizing plots!}\ntweets_full + labs(title= \"Mean tweets per journal per year\", x=\"Journal\", y=\"Mean # tweets\")\ntweets_full + theme_bw()\ntweets_full + theme_classic()\ntweets_full + theme_minimal()\n```\n\n",
    "created" : 1442332833708.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1880301279",
    "id" : "DFC90661",
    "lastKnownWriteTime" : 1442352471,
    "path" : "~/Learning_Things/altmetrics/Altmetrics_analyses.Rmd",
    "project_path" : "Altmetrics_analyses.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "type" : "r_markdown"
}